<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Partition Function Explosion</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                tags: 'none'
            },
            chtml: {
                fontURL: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2'
            },
            options: {
                renderActions: {
                    addMenu: []
                }
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <style>
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            height: 40vh;
            max-height: 450px;
            min-height: 300px;
        }

        .nav-btn.active {
            border-bottom: 2px solid #4f46e5;
            color: #4f46e5;
            font-weight: 600;
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>

<body class="bg-slate-50 text-slate-800 font-sans antialiased overflow-x-hidden">

    <!-- Chosen Palette: Slate gray background (bg-slate-50), Indigo accents (indigo-600) for primary actions, Emerald (emerald-500) for positive data/linear architectures, Rose (rose-500) for standard attention decay. Minimalist and academic. -->
    <!-- Application Structure Plan: A single-page interactive explainer divided into three main logical views (Overview, The Entropy Crisis, Architectural Solutions). This structure is chosen to guide the user sequentially from the abstract problem definition, to an interactive visualization of the core mathematical failure (Partition Explosion), and finally to how modern ML architectures solve it. Tabs are used for top-level navigation to keep the single page clean and prevent overwhelming the user with academic text all at once. -->
    <!-- Visualization & Content Choices: 
1. Goal: Inform about the core problem. Viz: Interactive Bar/Line Chart showing Attention Weights dropping as Context slider increases. Justification: Directly visualizes the abstract "Partition Function Explosion" equation. Method: Chart.js (Canvas).
2. Goal: Compare architectural solutions. Viz: Line chart comparing SNR over logarithmic Context Size. Justification: Demonstrates the practical impact of the math on different models (Standard vs Linear). Method: Chart.js (Canvas).
3. Goal: Explain Math. Viz: Tailwind CSS grid cards with MathJax. Justification: Keeps formulas readable while breaking down the Energy-Based Model framework into digestible pieces. Method: HTML/Tailwind. NO SVG/Mermaid. -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->

    <div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8">

        <header class="mb-10 text-center">
            <h1 class="text-3xl md:text-5xl font-extrabold text-slate-900 tracking-tight mb-4">The Partition Function
                Explosion</h1>
            <p class="text-xl text-slate-600 max-w-3xl mx-auto">An Energy-Based Analysis of Attention Decay in Large
                Language Models</p>
            <div class="mt-6 flex justify-center space-x-2">
                <span
                    class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-indigo-100 text-indigo-800">
                    Interactive Explainer
                </span>
                <span
                    class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-slate-200 text-slate-700">
                    Based on Academic Report
                </span>
            </div>
        </header>

        <nav class="flex justify-center border-b border-slate-200 mb-8">
            <button class="nav-btn active px-4 py-3 text-slate-500 hover:text-slate-700 transition-colors"
                data-target="view-overview">Overview & Theory</button>
            <button class="nav-btn px-4 py-3 text-slate-500 hover:text-slate-700 transition-colors"
                data-target="view-crisis">The Entropy Crisis</button>
            <button class="nav-btn px-4 py-3 text-slate-500 hover:text-slate-700 transition-colors"
                data-target="view-solutions">Architectural Solutions</button>
        </nav>

        <main id="main-content">

            <section id="view-overview" class="fade-in block">
                <div class="bg-white rounded-2xl shadow-sm border border-slate-200 p-6 md:p-8 mb-8">
                    <h2 class="text-2xl font-bold text-slate-900 mb-4">The Limit of Long Contexts</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-6">
                        This section introduces the foundational framework of the report. The report proposes that
                        standard softmax attention, the core mechanism of modern Transformers, fundamentally degrades
                        when processing extremely long sequences (over 32k tokens). By analyzing attention through the
                        lens of an Energy-Based Model (EBM), we uncover that this is not a training artifact, but a
                        mathematical inevitability known as the <strong>Partition Function Explosion</strong>.
                    </p>

                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mt-8">
                        <div class="bg-slate-50 p-6 rounded-xl border border-slate-100">
                            <div class="text-3xl mb-3">ðŸ”‹</div>
                            <h3 class="font-bold text-lg mb-2">Energy-Based Model</h3>
                            <p class="text-sm text-slate-600 mb-4">Attention is mapped to a Boltzmann distribution. The
                                dot product of Queries and Keys represents the negative "Energy" of a token
                                relationship.</p>
                            <div class="bg-white p-3 rounded text-center font-mono text-sm border border-slate-200">
                                $E(x_i, x_j) = -x_i W_Q W_K^T x_j$
                            </div>
                        </div>

                        <div class="bg-indigo-50 p-6 rounded-xl border border-indigo-100">
                            <div class="text-3xl mb-3">ðŸ’¥</div>
                            <h3 class="font-bold text-lg mb-2">The Partition Function</h3>
                            <p class="text-sm text-slate-600 mb-4">The denominator of the softmax function acts as the
                                partition function $Z$. As context length ($N$) increases, $Z$ grows linearly, adding
                                overwhelming noise.</p>
                            <div
                                class="bg-white p-3 rounded text-center font-mono text-sm border border-slate-200 overflow-x-auto">
                                $Z(x_i) = \sum_{j=1}^{N} e^{-E(x_i, x_j)}$
                            </div>
                        </div>

                        <div class="bg-slate-50 p-6 rounded-xl border border-slate-100">
                            <div class="text-3xl mb-3">ðŸ“‰</div>
                            <h3 class="font-bold text-lg mb-2">Attention Dilution</h3>
                            <p class="text-sm text-slate-600 mb-4">Because $Z$ grows with $N$, the probability mass
                                assigned to the actual target "signal" token shrinks, causing the model to "forget" or
                                ignore distant context.</p>
                            <div class="bg-white p-3 rounded text-center font-mono text-sm border border-slate-200">
                                $P(x_j | x_i) = \frac{e^{-E}}{Z(x_i)}$
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="view-crisis" class="fade-in hidden">
                <div class="bg-white rounded-2xl shadow-sm border border-slate-200 p-6 md:p-8">
                    <h2 class="text-2xl font-bold text-slate-900 mb-4">Interactive: The Entropy Crisis Simulator</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        Interact with the slider below to simulate adding more tokens to a language model's context
                        window. This visualization demonstrates the <strong>Partition Function Explosion</strong>. Even
                        if a "Signal Token" has a very high mathematical relevance (high energy) compared to "Noise
                        Tokens", as the sheer volume of noise tokens increases, the Partition Function $Z$ swells. This
                        flattens the resulting probability distribution, leading to maximum entropy and rendering the
                        model unable to confidently select the correct information.
                    </p>

                    <div class="bg-slate-900 rounded-xl p-6 text-white mb-8">
                        <div class="flex flex-col md:flex-row justify-between items-center mb-6">
                            <div class="mb-4 md:mb-0">
                                <label for="context-slider"
                                    class="block text-sm font-medium text-slate-300 mb-2">Context Window Size ($N$
                                    tokens)</label>
                                <input type="range" id="context-slider" min="10" max="100000" value="10" step="10"
                                    class="w-full md:w-64 accent-indigo-500">
                            </div>
                            <div class="text-center md:text-right">
                                <div class="text-sm text-slate-400 uppercase tracking-wide">Current $N$</div>
                                <div class="text-3xl font-bold text-indigo-400" id="context-display">10</div>
                            </div>
                            <div class="text-center md:text-right">
                                <div class="text-sm text-slate-400 uppercase tracking-wide">Partition Function $Z$</div>
                                <div class="text-3xl font-bold text-rose-400" id="z-display">~2.2e4</div>
                            </div>
                        </div>

                        <div class="chart-container bg-slate-800 rounded-lg p-4 border border-slate-700">
                            <canvas id="entropyChart"></canvas>
                        </div>

                        <div class="mt-4 grid grid-cols-2 gap-4 text-center text-sm">
                            <div class="bg-slate-800 p-3 rounded border border-slate-700">
                                <span class="block text-slate-400">Signal Token Attention Weight</span>
                                <span id="signal-weight-display" class="font-bold text-lg text-emerald-400">99.9%</span>
                            </div>
                            <div class="bg-slate-800 p-3 rounded border border-slate-700">
                                <span class="block text-slate-400">Average Noise Token Weight</span>
                                <span id="noise-weight-display" class="font-bold text-lg text-rose-400">0.01%</span>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="view-solutions" class="fade-in hidden">
                <div class="bg-white rounded-2xl shadow-sm border border-slate-200 p-6 md:p-8">
                    <h2 class="text-2xl font-bold text-slate-900 mb-4">Architectural Solutions & SNR Decay</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        This section analyzes how different neural network architectures attempt to solve the Partition
                        Function Explosion. The report highlights that standard mechanisms suffer from severe
                        Signal-to-Noise Ratio (SNR) decay. By interacting with the chart, you can compare how Standard
                        Softmax, Differential Attention (which cancels out common noise), and Linear Attention (which
                        completely removes the exponential softmax) maintain their signal integrity as context scales.
                    </p>

                    <div class="flex flex-col lg:flex-row gap-8">
                        <div class="w-full lg:w-2/3">
                            <div class="chart-container">
                                <canvas id="snrChart"></canvas>
                            </div>
                            <div class="flex flex-wrap justify-center gap-4 mt-6">
                                <button
                                    class="toggle-btn bg-rose-100 text-rose-800 px-3 py-1 rounded-full text-sm font-semibold border border-rose-200 hover:bg-rose-200 transition"
                                    data-dataset="0">Toggle Standard Softmax</button>
                                <button
                                    class="toggle-btn bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-semibold border border-blue-200 hover:bg-blue-200 transition"
                                    data-dataset="1">Toggle Differential Attention</button>
                                <button
                                    class="toggle-btn bg-emerald-100 text-emerald-800 px-3 py-1 rounded-full text-sm font-semibold border border-emerald-200 hover:bg-emerald-200 transition"
                                    data-dataset="2">Toggle Linear Attention</button>
                            </div>
                        </div>

                        <div class="w-full lg:w-1/3 flex flex-col gap-4">
                            <div class="p-4 border-l-4 border-rose-500 bg-slate-50 rounded-r-lg">
                                <h4 class="font-bold text-slate-900">Standard Softmax</h4>
                                <p class="text-sm text-slate-600 mt-1">Experiences $O(1/\sqrt{N})$ or worse SNR decay.
                                    Fails to retrieve facts robustly beyond its training window.</p>
                            </div>
                            <div class="p-4 border-l-4 border-blue-500 bg-slate-50 rounded-r-lg">
                                <h4 class="font-bold text-slate-900">Differential Attention</h4>
                                <p class="text-sm text-slate-600 mt-1">Uses two softmax heads and subtracts them. This
                                    acts as a noise-cancellation mechanism, bounding the growth of $Z$ and delaying the
                                    entropy crisis.</p>
                            </div>
                            <div class="p-4 border-l-4 border-emerald-500 bg-slate-50 rounded-r-lg">
                                <h4 class="font-bold text-slate-900">Linear Attention</h4>
                                <p class="text-sm text-slate-600 mt-1">Models like RWKV or RetNet drop the softmax
                                    entirely, relying on linear projections and element-wise operations. They avoid the
                                    partition function entirely, yielding stable long-context SNR.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

        </main>
    </div>

    <script>
        const navButtons = document.querySelectorAll('.nav-btn');
        const views = document.querySelectorAll('main > section');

        navButtons.forEach(btn => {
            btn.addEventListener('click', () => {
                navButtons.forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const targetId = btn.getAttribute('data-target');
                views.forEach(view => {
                    if (view.id === targetId) {
                        view.classList.remove('hidden');
                    } else {
                        view.classList.add('hidden');
                    }
                });

                if (targetId === 'view-crisis' && entropyChartInstance) {
                    entropyChartInstance.resize();
                }
                if (targetId === 'view-solutions' && snrChartInstance) {
                    snrChartInstance.resize();
                }
            });
        });

        const E_signal = 10;
        const E_noise = 0;

        const ctxEntropy = document.getElementById('entropyChart').getContext('2d');
        let entropyChartInstance = new Chart(ctxEntropy, {
            type: 'bar',
            data: {
                labels: ['Signal Token', 'Noise Token 1', 'Noise Token 2', 'Noise Token 3', '... Average Noise'],
                datasets: [{
                    label: 'Attention Weight Probability',
                    data: [0, 0, 0, 0, 0],
                    backgroundColor: [
                        'rgba(52, 211, 153, 0.8)',
                        'rgba(244, 63, 94, 0.4)',
                        'rgba(244, 63, 94, 0.4)',
                        'rgba(244, 63, 94, 0.4)',
                        'rgba(244, 63, 94, 0.4)'
                    ],
                    borderColor: [
                        'rgba(52, 211, 153, 1)',
                        'rgba(244, 63, 94, 1)',
                        'rgba(244, 63, 94, 1)',
                        'rgba(244, 63, 94, 1)',
                        'rgba(244, 63, 94, 1)'
                    ],
                    borderWidth: 1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    legend: { display: false },
                    tooltip: {
                        callbacks: {
                            label: function (context) {
                                return (context.raw * 100).toFixed(4) + '%';
                            }
                        }
                    }
                },
                scales: {
                    y: {
                        beginAtZero: true,
                        max: 1.0,
                        grid: { color: 'rgba(255, 255, 255, 0.1)' },
                        ticks: {
                            color: 'rgba(255, 255, 255, 0.7)',
                            callback: function (value) { return (value * 100) + '%'; }
                        }
                    },
                    x: {
                        grid: { display: false },
                        ticks: { color: 'rgba(255, 255, 255, 0.7)' }
                    }
                },
                animation: { duration: 200 }
            }
        });

        const slider = document.getElementById('context-slider');
        const contextDisplay = document.getElementById('context-display');
        const zDisplay = document.getElementById('z-display');
        const signalWeightDisplay = document.getElementById('signal-weight-display');
        const noiseWeightDisplay = document.getElementById('noise-weight-display');

        function updateEntropyCrisis() {
            const N = parseInt(slider.value);
            contextDisplay.innerText = N.toLocaleString();

            const Z = Math.exp(E_signal) + (N - 1) * Math.exp(E_noise);

            zDisplay.innerText = Z > 10000 ? Z.toExponential(2) : Z.toFixed(0);

            const p_signal = Math.exp(E_signal) / Z;
            const p_noise = Math.exp(E_noise) / Z;

            signalWeightDisplay.innerText = (p_signal * 100).toFixed(2) + '%';
            noiseWeightDisplay.innerText = Z > 10000 ? (p_noise * 100).toExponential(2) + '%' : (p_noise * 100).toFixed(4) + '%';

            entropyChartInstance.data.datasets[0].data = [p_signal, p_noise, p_noise, p_noise, p_noise];
            entropyChartInstance.update();
        }

        slider.addEventListener('input', updateEntropyCrisis);
        updateEntropyCrisis();

        const ctxSnr = document.getElementById('snrChart').getContext('2d');
        const contextLengths = [1000, 4000, 16000, 32000, 64000, 128000];

        const snrStandard = contextLengths.map(n => 100 / Math.sqrt(n));
        const snrDiff = contextLengths.map(n => 100 / Math.pow(n, 0.2));
        const snrLinear = contextLengths.map(n => 80);

        let snrChartInstance = new Chart(ctxSnr, {
            type: 'line',
            data: {
                labels: contextLengths.map(n => (n / 1000) + 'k'),
                datasets: [
                    {
                        label: 'Standard Softmax',
                        data: snrStandard,
                        borderColor: '#f43f5e',
                        backgroundColor: 'rgba(244, 63, 94, 0.1)',
                        borderWidth: 3,
                        tension: 0.4,
                        fill: true
                    },
                    {
                        label: 'Differential Attention',
                        data: snrDiff,
                        borderColor: '#3b82f6',
                        backgroundColor: 'rgba(59, 130, 246, 0.1)',
                        borderWidth: 3,
                        tension: 0.4,
                        fill: false
                    },
                    {
                        label: 'Linear Attention',
                        data: snrLinear,
                        borderColor: '#10b981',
                        backgroundColor: 'rgba(16, 185, 129, 0.1)',
                        borderWidth: 3,
                        tension: 0.4,
                        borderDash: [5, 5],
                        fill: false
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    legend: { position: 'top' },
                    tooltip: {
                        mode: 'index',
                        intersect: false,
                        callbacks: {
                            label: function (context) {
                                return context.dataset.label + ': ' + context.raw.toFixed(2) + ' SNR';
                            }
                        }
                    }
                },
                scales: {
                    y: {
                        title: { display: true, text: 'Signal-to-Noise Ratio (Relative)' },
                        min: 0
                    },
                    x: {
                        title: { display: true, text: 'Context Window Size (Tokens)' }
                    }
                },
                interaction: {
                    mode: 'nearest',
                    axis: 'x',
                    intersect: false
                }
            }
        });

        const toggleBtns = document.querySelectorAll('.toggle-btn');
        toggleBtns.forEach(btn => {
            btn.addEventListener('click', function () {
                const datasetIndex = this.getAttribute('data-dataset');
                const meta = snrChartInstance.getDatasetMeta(datasetIndex);

                meta.hidden = meta.hidden === null ? !snrChartInstance.data.datasets[datasetIndex].hidden : null;

                if (meta.hidden) {
                    this.style.opacity = '0.5';
                } else {
                    this.style.opacity = '1';
                }

                snrChartInstance.update();
            });
        });

    </script>
</body>

</html>